\documentclass[12pt]{report}

\usepackage{classTools}
\usepackage{ialib}
\input{stats-import}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{pgfplots}

\input{preamble}
\input{macros}
\input{letterfonts}

\hfuzz=5.0pt
\usepackage[parfill,indent]{parskip} % Removes paragraph indentation and adds space between paragraphsimage.png

%\title{\Huge{Some Class}\\Random Examples}
%\author{\huge{Your Name}}
\title{%
    \huge{Introduction to Data Compression} \\
    \large{\large{A Linear Algebra Approach}}
}
\author{\large{Layan Ansari, Adam Pearl, IÃ±aki Arango}}
\date{\today}

\begin{document}
    \maketitle
    
    %\begin{abstract}
    %    \blindtext[3]
    %    \Inote{Consider removing abstract}
    %\end{abstract}
    
    \newpage% or \cleardoublepage
    % \pdfbookmark[<level>]{<title>}{<dest>}
    \pdfbookmark[section]{\contentsname}{toc}
    
    % ================== %
    % == Cover Letter == %
    % ================== %
    \vspace*{1cm}
    \centerline{\huge\huge Cover Letter}
    \vspace*{1cm}

    (We will write something here for the final product). Reflect on challenges and successes in the cover letter; what you planned to accomplish but couldn't; show that you put in effort and that you took a lot away from this project; reflect on what you learned while doing this project and how it may have helped you discover a new passion or delve into a passion you already had; thank whoever did your peer review.
    
    \pagebreak
    
    % ======================= %
    % == Table of Contents == %
    % ======================= %
    \tableofcontents
    
    \pagebreak

    % ============== %
    % == Contents == %
    % ============== % 
    \chapter{Introduction}
        Data flows everywhere, it is in every product that we interact with daily. More than 2.5 quintillion bytes are created every day (that is 2.5 followed by 18 zeros) \cite{alexaqz_2015}. For the next 5 years data is expected to grow by 40\% every year \cite{marr_2022}.

        We need, and will continue to need, more powerful and faster computers capable of processing this increasing amount of information, and better storage systems to safekeep it.

        There has been a lot of improvement hardware-wise in the last couple of decades, which increased the density of our storage systems. However, what if we could improve our information storage density through other, non-physical, means? What if we could store the same amount of ``information'' but with fewer ``bits''? This would allow us to work in parallel with scientists researching physical systems.
        
        This is where data compression comes in. It allows us to store the same amount of ``information'' in less bits, digits, characters, or whatever the most basic unit of storage is in our medium of choice. An early example of compression work is the Morse Code, which was invented in 1838 for use in the telegraph industry. Telegraph bandwidth was scarce, so shorter code words were assigned for common letters in the English alphabet, such as ``e'' and ``t'', that would be sent more often \cite{Wolfram2002}.
    
    \chapter{Tools for Compression}
        There are various tools at our disposal that we can use to study and perform data compression. Many of the common techniques that are used today are the outcome of collaboration between scientists and engineers focused on different branches of Math, such as Probability, Statistics, Information Theory, and Linear Algebra.
        
        While we have focused on Linear Algebra this semester, it is necessary to know some introductory concepts from these other branches to understand basic data compression. In this section we will introduce these branches and explain some of their basic concepts.
        
        \section{Probability \& Statistics}
            Probability is the study of the the likelihood of events, independently and given the occurrence of other events (e.g., given that it is cloudy, what is the probability that it will rain today?). Statistics is the discipline that concerns the collection, organization, analysis, interpretation, and presentation of data \cite{wiki:Statistics}.

            \subsection{Sample Space}
                The most basic concept in probability is that of the \textbf{sample space} and \textbf{events}.
                \dfn{Sample Space}{
                    The \textbf{sample space} is the set of all possible outcomes that an experiment can have.
                }
                \ex{Flipping a coin}{
                    If we flip a coin then the sample space is $S = \{\text{Heads}, \text{Tails}\}$.
                }
                \ex{Rolling a die}{
                    If we roll a 6-sided die the sample space is $S = \{1, 2, 3, 4, 5, 6\}$.
                }
                
            \subsection{Events}
                \dfn{Event}{
                    \textbf{Events} are then mathematically defined as a subset of the sample space.
                }
                \ex{Rolling a die}{
                    For example, in the 6-sided die example let $M$ be the event we roll a 4 or greater. Then $M = \{4, 5, 6\} \subseteq S$.
                }

            \subsection{Probability}
                \dfn{Probability}{
                    The \textbf{probability} of an event (denote $P(M)$ for an event $M$) is the measure of how likely the event is to occur as the outcome of a random experiment.

                    Probability is measured out of 1, which means that events certain to occur have a probability of 1 and events that will never occur have a probability of 0. 
                    
                    Naturally, the probability of the sample space $P(S) = 1$, since, by definition, the outcome of the experiment must be one in the sample space.
                }
            
            \subsection{Random Variables}
                \dfn{Random Variables}{
                    \textbf{Random variables} (also r.v.'s) are functions that map the outcome of a random experiment (in the sample space) to a measurable quantity (e.g., real number) or a mathematical object. When the experiment is performed, and the outcome is mapped to a value, we say the random variable ``crystallizes'' to that value. The set of all values a random variable crystallizes to is called the support, denoted $R_X$ for a r.v. $X$. A specified random variable crystallizing to a certain value is an event.
                }
                \nt{
                    We normally denote random variables with capital letters and their crystallized values with lower case letters.
                }
                \ex{Flipping a coin}{
                    Let us flip a fair coin with equal probabilities of landing heads or tails. The sample space is then $S = \{H, T\}$ with $P(\{H\}) = P(\{T\}) = 0.5$.

                    Let $X$ be a random variable that crystallizes to -1 if the coin lands Heads and to 1 if the coin lands Tails. Then $X = -1$ and $X = 1$ are events and we write $P(X = -1) = P(\{H\})$ and $P(X = 1) = P(\{T\})$.
                }

            \subsection{Expectation}
                \dfn{Expectation}{
                    The \textbf{expectation} of a random variable $X$ with, denoted $\IAexpected(X)$ is the the average value that the random variable will hold weighted by the probability of the variable crystallizing to that value. If there is no ambiguity over what variable we are taking about, we use the letter $\mu = \IAexpected(X)$. Its formula is
                        \[\IAmean{X} = \IAexpected(X) = \sum_{x \in R_X} x \cdot P(X = x)\]
                }

            \Inote{Remove any of these definitions if they are not needed during the paper}
            
            \subsection{Variance}
                \dfn{Variance}{
                    \textbf{Variance} is a measure of spread. It measures how far the values in the support of an r.v. are to the mean. For a r.v. $X$ we denote its variance as $\var(X)$. The formula for variance is
                        \[\var(X) = \IAexpected(X^2) - [\IAexpected(X)]^2\]
                }
                \dfn{Standard Deviation}{
                    Its cousin, \textbf{standard deviation}, is defined as $\sigma = \sqrt{\var(X)}$, and is used because it is sometimes more practical to express the spread in terms of standard deviation.
                }

            
            \subsection{Covariance}
                \dfn{Covariance}{
                    While the variance is the measure of spread for a single variance, the \textbf{covariance} is the measure of joint between two random variables \cite{Rice2007}. 
                }
                \nt{
                    If greater values of one variable correspond to greater values of the other variable covariance is positive. If greater values of one variable correspond to lesser values of the other variable, covariance is negative.
                }

                Below is a graphical comparison of what negative, zero and positive covariance looks like between two random variables. Each point encodes one instance of the two variables crystallized together.

                \begin{figure}[H]
                    \centering
                    \begin{minipage}{0.3\textwidth}
                        \centering
                        \begin{tikzpicture}
                            \begin{axis}[
                                width=2.25in,
                                height=2.25in,
                                xmin=-1,
                                xmax=5,
                                ymin=-1,
                                ymax=5,
                                axis lines=middle,
                                xlabel={$X$},
                                ylabel={$Y$}]
                                \addplot[mark=*] coordinates {(1,4)} node(A){};
                                \addplot[mark=*] coordinates {(2,3)};
                                \addplot[mark=*] coordinates {(2,2)};
                                \addplot[mark=*] coordinates {(3,1)};
                                \addplot[mark=*] coordinates {(4,1)} node(B){};
                                \addplot[mark=*] coordinates {(4,2)};
                                \addplot[mark=*] coordinates {(3,3)};
                                \addplot[mark=*] coordinates {(2,4)};
                                \node (C)[very thick,draw=blue!75!black,ellipse,rotate fit=40,fit=(A) (B),fill=blue,opacity=0.1] {};
                            \end{axis}
                        \end{tikzpicture}
                        $\cov(X, Y) < 0$
                        \label{fig:negative_cov}
                    \end{minipage}
                    \begin{minipage}{0.3\textwidth}
                        \centering
                        \begin{tikzpicture}
                            \begin{axis}[
                                width=2.25in,
                                height=2.25in,
                                xmin=-1,
                                xmax=5,
                                ymin=-1,
                                ymax=5,
                                axis lines=middle,
                                xlabel={$X$},
                                ylabel={$Y$}]
                                \addplot[mark=*] coordinates {(1.5,2.5)};
                                \addplot[mark=*] coordinates {(2,3)};
                                \addplot[mark=*] coordinates {(3,2)} node(B){};
                                \addplot[mark=*] coordinates {(2,1)};
                                \addplot[mark=*] coordinates {(2,2)};
                                \addplot[mark=*] coordinates {(2,3)};
                                \addplot[mark=*] coordinates {(1,2)} node(A){};
                                \addplot[mark=*] coordinates {(2.75,1.75)};
                                \node (C)[very thick,draw=blue!75!black,ellipse,rotate fit=40,fit=(A) (B),fill=blue,opacity=0.1] {};
                            \end{axis}
                        \end{tikzpicture}
                        $\cov(X, Y) = 0$
                        \label{fig:zero_cov}
                    \end{minipage}
                    \begin{minipage}{.3\textwidth}
                        \centering
                        \begin{tikzpicture}
                            \begin{axis}[
                                width=2.25in,
                                height=2.25in,
                                xmin=-1,
                                xmax=5,
                                ymin=-1,
                                ymax=5,
                                axis lines=middle,
                                xlabel={$X$},
                                ylabel={$Y$}]
                                \addplot[mark=*] coordinates {(1,1)} node(A){};
                                \addplot[mark=*] coordinates {(2,2)};
                                \addplot[mark=*] coordinates {(2,3)};
                                \addplot[mark=*] coordinates {(3,4)};
                                \addplot[mark=*] coordinates {(4,4)} node(B){};
                                \addplot[mark=*] coordinates {(4,3)};
                                \addplot[mark=*] coordinates {(3,2)};
                                \addplot[mark=*] coordinates {(2,1)};
                                \node (C)[very thick,draw=blue!75!black,ellipse,rotate fit=-40,fit=(A) (B),fill=blue,opacity=0.1] {};
                            \end{axis}
                        \end{tikzpicture}
                        $\cov(X, Y) > 0$
                        \label{fig:positive_cov}
                    \end{minipage}%
                \end{figure}
                
            
        
        \section{Information Theory (maybe could be skipped)}
        % ===== Put the brief history here =====
        \subsection{Strictly required math/algebra information theory background}
            \Inote{Fill out with definitions or content if it is needed for our future proofs}
    
    \chapter{Start of the Story: What do we want to compress?}
        \section{Handwriting Recognition}
            Communication is an essential part of relating to people, and one of the oldest and most accessible methods of communication within a given language is writing by hand. In spite of the major effort that has been expended to bring about a paper-free society, a very large number of paper-based documents are processed daily by computers all over the world in order to handle, retrieve, and store information. The problem is that the manual process used to enter the data from these documents into computers demands a great deal of time and money (Bortolozzi, de Souza Britto Jr., Oliveira and Morita, n.d.)\Lnote{put in citation later}. These documents may need to be processed for a number of reasons, among them historical documentation (e.g. digitally documenting culturally and historically significant documents and scripts, which until recently were more often than not handwritten or on print paper), recognition for medical prescriptions, or for tablet soft-wares to convert usersâ handwriting into digital text.
        
            Thus, the task of handwriting recognition is the transcription of handwritten data into a digital format, and this task obviously benefits from data compression. The goal is to process handwritten data electronically with the same or nearly the same accuracy as humans (Gunter, n.d)\Lnote{citation}.
            
            Basically, handwriting can be divided into two categories, cursive script and printed handwriting. Accuracy is the main problem in handwriting recognition for both categories because of the similar strokes and shapes some letters may possess. The software may have an inaccurate recognition of the letter, considering the possibility of the handwriting being illegible or some other factors (\Lnote{citation}). One notable problem that makes this task difficult especially in cursive handwriting recognition is the fact that there may be no obvious character boundaries (the start and end of a character); compared to printed handwriting, it does not have gaps or spaces between each letter to know the start and stop of recognition per character (\Lnote{citation}). This issue is compounded for languages like Arabic, where cursive is the only form of script and there exist "shortcuts" to further simplify the cursive script and make writing more fluid (e.g. removing the "dots"/accent marks that exist above/below certain letters).
            
            This is where the data compression/dimension reduction and eigenface technique comes into play! In essence, if we have a large data set that consists of thousands or even millions of images of words (probably limited to one language), we want to find a way to recognize patterns in these images. From these patterns, we can then determine which ones have the most âimportanceâ and attempt to express these images as a weighted combination of these most important patterns (and thus in a lower dimension).
        
    
    \chapter{Principal Component Analysis}
        \section{Extract ``high information'' regions in the data space}
            \Inote{Use our variance knowledge from example image to say that some sections of the image have less information (the constant ones) and some have more information (the ones that vary widely)}
            \begin{figure}[H]
                \centering
                \includegraphics[width=0.7\textwidth]{mnist-handwriting-dataset.jpg}
                \caption{\label{fig:mnist-handwriting-dataset} Samples of the numbers 5, 0, 4, and 1 from the MNIST handwriting dataset.}
            \end{figure}
        \section{Quantify the intuitive findings from the information theory subsection}
            \Inote{If we write about info theory write the entropy of the regions of the image here}
        
        \section{Formalizing the calculation of ``high information'' eigenvectors}
            \Lnote{change wording so that this applies to handwriting recognition instead of face recognition}
            The motivation for using principal component analysis is to find what Turk and Pentland referred to as "face space," and which in our case we can conceptualize as the data space of letters. The face images, as previously explained, live as vectors in large dimensional spaces ($65,536$-dimensional space for a typical image). The goal at this stage is to reduce our large input vector space to a lower dimensional subspace that can be described be an eigenbasis.  
    
            \dfn{Covariance}{
                In order to construct... we need to define the \textbf{covariance}. Covariance measures the overall correlation between certain variables. The covariance $\sigma$ of two variables, $X$ and $Y$, is defined
                    \[\sigma(X,Y) = \frac{1}{M} \sum _{i=1}^{M} (X_i-\IAmean{X}) (Y_i-\IAmean{Y})\]
            }
    
            \dfn{Covariance Matrix}{
                \textbf{Covariance matrix}, then, is a matrix that measures the covariance of various variables.
                
                For variables $x_1,\ldots,x_p$, the covariance matrix is defined as follows
                \[
                    C = \IAmatrix{
                        \sigma_{11} & \sigma_{12} & ... & \sigma_{1p} \\
                        \sigma_{21} & \sigma_{22} &  & \\
                        \vdots  &  & \ddots  & \\
                        \sigma_{p1} &  &  & \sigma _{pp}
                    }
                \]
                
                where $\sigma_{ij}$ represents the covariance between the $i^{th}$ and $j^{th}$ variable. In the case of eigenfaces, the $p$ independent variables correspond to our $N^{2}$ bits, and thus $p$ corresponds to the dimension of our images. 
            }
            
            Having thus defined our covariance matrix, we must now discuss why we are taking the eigenvectors of this particular matrix to be our eigenfaces. In order for out eigenfaces to be as efficient as possible, we want we want each eigenface to encode as much variance as possible. 
            
            We take eigenvectors to simplify a relation defined by matrix multiplication to a relation defined by scalar multiplication, so that we may write any face as a linear combination of eigenvectors. Taking an eigenvector $e$, $C e = \lambda e$. If $\lambda$ is large, then the covariance between $e$ and all many other faces is high. If $\lambda$ is small, then the covariance is less. Therefore, we will see later that we want to prioritize large eigenvalues when constructing our eigenbasis. 
            
            Before we discuss the eigenvalues, however, we must derive some more definitions of $C$.
            
            Firstly, given the large size of $N^2$, it is convenient for us to find a more condensed description of the covariance matrix
    
            \thm{}{
                Let $C$ be a $p \times p$ covariance matrix where each element $c_{ij} = \sigma_{ij}$. Let $\Phi_a = P_a - \IAmean{P}$, where
                    \[
                        P_a = \IAmatrix{
                            x_{1a} \\
                            x_{2a} \\
                            \vdots \\
                            x_{pa}
                        }
                    \]
                with $a$ from $1$ to $M$.
                
                Then $C = \frac{1}{M} \sum_{a=1}^M \Phi_a \Phi_a^T$.
            }
            \begin{myproof}
                We can prove this theorem starting with
                    \[
                        C = \IAmatrix{
                            \sigma_{11} & \sigma_{12} & \cdots & \sigma_{1p} \\
                            \sigma_{21} & \sigma_{22} &        &             \\
                            \vdots      &             & \ddots &             \\
                            \sigma_{p1} &             &        & \sigma_{pp}
                        }
                    \]
        
                From the definition of covariance, each entry $c_{ij} = \frac{1}{M} \sum _{i=1}^M \left( x_{ia} - \IAmean{x_i}\right) \left(x_{ia} - \IAmean{x_i}\right)$. Since matrices add linearly, we can factor out the summation $\frac{1}{M} \sum_{i=1}^M$. Thus we can write
                    \[
                        C = \frac{1}{M} \sum_{i=1}^M \IAmatrix{
                            \left(x_{1a} - \IAmean{x}_1\right) \left(x_{1a} - \IAmean{x}_1\right) & \dotsc  & \left(x_{1a} - \IAmean{x}_1\right) \left(x_{pa} - \IAmean{x}_p\right) \\
                            \vdots & \ddots & \\
                            \left(x_{pa} - \IAmean{x}_p\right) \left(x_{1a} - \IAmean{x}_1\right) &  & \left(x_{pa} - \IAmean{x}_p\right) \left(x_{pa} - \IAmean{x}_p\right)
                        }
                    \] \Inote{Clarify notation because $\IAmean{x}_p$ is not defined anywhere}
                
                
                Now consider the term $\Phi_a \Phi_a^T = (P_i - \IAmean{P}) (P_i - \IAmean{P})^T$, as defined in the theorem. Since the transpose operator is distributive,
                    \[
                        (P_i - \IAmean{P}) (P_i - \IAmean{P})^T =
                            \left(
                                \IAmatrix{
                                    x_{1a} \\
                                    x_{2a} \\
                                    \vdots \\
                                    x_{pa}
                                } - \IAmean{P}
                            \right) \left(
                                \IAmatrix{
                                    x_{1i} & x_{2i} & \dotsc & x_{pi}
                                } - \IAmean{P}^T
                            \right)
                    \]
                    \[
                        =
                            \left(
                                \IAmatrix{
                                    x_{1a} \\
                                    x_{2a} \\
                                    \vdots \\
                                    x_{pa}
                                } - \IAmatrix{
                                    \IAmean{x}_1 \\
                                    \IAmean{x}_2\\
                                    \vdots \\
                                    \IAmean{x}_p
                                }
                            \right) \left(
                                \IAmatrix{
                                    x_{1a} & x_{2a} & \dotsc & x_{pa}
                                } - \IAmatrix{
                                    \IAmean{x}_1 & \IAmean{x}_2 & \dotsc & \IAmean{x}_p
                                }
                            \right)
                    \]
                    \[
                        =
                            \IAmatrix{
                                x_{1a} - \IAmean{x}_1 \\
                                x_{2a} - x_2 \\
                                \vdots \\
                                x_{pa} - \IAmean{x}_p
                            }
                            \IAmatrix{
                                x_{1a} - \IAmean{x}_1 & x_{2a} - \IAmean{x}_2 & \dotsc & x_{pa} -\IAmean{x}_p
                            }
                    \]
                
                Therefore,
                    \[
                        \Phi_a {\Phi_a}^T = \IAmatrix{
                            \left(x_{1a} - \IAmean{x}_1\right)\left(x_{1a} - \IAmean{x}_1\right) & \dotsc & \left(x_{1a} - \IAmean{x}_1\right) \left(x_{pa} - \IAmean{x}_p\right) \\
                            \vdots & \ddots & \\
                            \left(x_{pa} - \IAmean{x}_p\right) \left(x_{1a} - \IAmean{x}_1\right) & & \left(x_{pa} - \IAmean{x}_p\right) \left(x_{pa} - \IAmean{x}_p\right)
                        }
                    \]
                    \[
                        C = \frac{1}{M} \sum_{a=1}^M \Phi_a {\Phi_a}^T
                    \]
            \end{myproof}
            
            By constructing 
            
            \href{https://builtin.com/data-science/step-step-explanation-principal-component-analysis}{REFERENCE PRINCIPAL COMPONENT ANALYSIS}
            
            In the eigenface case, we wish to measure the difference between our face vectors $\Gamma_i$ differ from the average face. We can define the average face $\IAmean{\Gamma}$ as follows,
                \[
                    \IAmean{\Gamma} = \frac{1}{M} \sum_{n=1}^M \Gamma_n
                \]
    
            We can write the difference $\Phi_i$ between the average face and the $i^{th}$ face,
                \[
                    \Phi_i = \Gamma_i - \IAmean{\Gamma}
                \]
            
            \href{https://nzmaths.co.nz/category/glossary/variance-discrete-random-variable\#:\textasciitilde:text=A\%20measure\%20of\%20spread\%20for,2\%20or\%20\%CF\%832x.}{Hello}
            
            \thm{}{
                Let $C$ be an $N^2 \times N^2$ covariance matrix, defined as $C = \frac{1}{M} \sum_{n=1}^M \Phi_n \Phi_n^T$, where $M$ is the dimension of the domain $C$. Let $A$ be an $N^2 \times N^2$ matrix with columns $[\Phi_1 \ \Phi_2 \ \dots \Phi_M]$.
                
                Then $C = A A^T$.
            }
    
            \begin{myproof}
                
            \end{myproof}
            
            Going back to our matrix $C$,
                \[
                    C = \IAmatrix{
                        \sigma_{11} & \sigma_{12} & \dots  & \sigma_{1p} \\
                        \sigma_{21} & \sigma_{22} &        &             \\
                        \vdots      &             & \ddots &             \\
                        \sigma_{p1} &             &        & \sigma_{pp}
                    }
                \]
    
             We can expand $C$ according to the definition of $\sigma$,
                \[
                    C = \frac{1}{M} \sum_{a=1}^M \IAmatrix{
                        \Phi_1 {\Phi_1}^T & \Phi_1 \Phi_2^T   & \dots  & \Phi_1 {\Phi_p}^T \\
                        \Phi_2 {\Phi_1}^T & \Phi_2 {\Phi_2}^T &        &                   \\
                        \vdots            &                   & \ddots &                   \\
                        \Phi_p {\Phi_1}^T &                   &        & \Phi_p {\Phi_p}^T
                    }
                \]
        
            \Anote{...}
            
            Now we can express our eigenvectors $v_{i}$ and eigenvalues $\lambda _{i}$ as
                \[
                    \lambda_i v_i = A A^T v_i
                \]
    
            However, $A A^T$ is an $N^2$by $N^2$ matrix (where $N \times N$ is the resolution of the image), which for images of any discernable resolution will be much larger than desired in order to compute its eigenvectors. 
            
            Let
                \[
                    S = A A^T = \lambda_i v_i
                \]
    
            Then,
                \[
                    S^T = \left(A A^T\right)^T = A^T A
                \]
    
            We can show that $S$ and $S^{T}$ have the same eigenvalues. 
            
            Since the indetity matrix is symmetrical, since the transpose operation is distributive, and since $\det(A) = \det\left(A^T\right)$,
                \[
                    \det\left(S^T - \lambda_i I\right) = \det(S - \lambda_i I)^T = \det(S - \lambda_i I)
                \]
            and therefore $S$ and $S^T$ have the same eigenvalues.
            
            Applying this to our eigenfaces,
                \[
                    S^T = A^T A = \lambda_i u_i
                \]
    
            The advantage of effectively reordering the $A$ and $A^T$ is that our new matrix $S^T$ is an $M \times M$ matrix. $M$ corresponds to the size of the training set, which in most cases is much smaller than the dimension of the resolution of the image. Therefore, it will be much easier to compute the eigenfaces. We can then write each face as a linear combination of the eigenfaces
            
            \href{https://datascienceplus.com/understanding-the-covariance-matrix/}{MEGAREFERENCE}
            
            \[
                \Gamma_i = \sum_{n=1}^M c_{in} u_{n}
            \]
    
            We can interpret the eigenvectors of the covariance matrix as vectors that encode the greatest variance in data, and as such are the most suited for reconstructing faces from (ghosts). The corresponding eigenvalues tell how much variance each eigenvector encodes. For this reason, the greater the eigenvalue the more adaptable and better suited the eigen vector. When choosing eigenvectors to form a basis for the eigenspace used for facial reckognition, one therefore prioritizes them by eigenvalue from highest to lowest.
            
            \href{http://math.clarku.edu/\textasciitilde djoyce/ma217/covar.pdf}{AWDAWD}
    
    \section{Coding example perhaps?}
    \blindtext
    
    \chapter{Singular Value Decomposition}
    \section{Explain how an alternative way to look at this is through SVD}
    
    \chapter{Uses with Compression}
    \section{XYZ storage}
    \blindtext[1]
    \section{images (medical, handrwiting, S\\XYZ), sound (voice recognition)}
    \blindtext[1]
    
    \chapter{Uses to optimize and enable new kinds of algorithms (fingerprint detection)}
    \section{XYZ recognition}
    \blindtext[1]
    \section{Facial and handrwiting detection algorithms, voice recognition algorithms}
    \blindtext[1]

    \chapter{Discussion}
    \section{Is this software Anglo-centric?}
    \blindtext[1]
    
    \pagebreak

    \chapter{}
    \section{Random Examples}
    \dfn{Limit of Sequence in $\bs{\bbR}$}{Let $\{s_n\}$ be a sequence in $\bbR$. We say $$\lim_{n\to\infty}s_n=s$$ where $s\in\bbR$ if $\forall$ real numbers $\eps>0$ $\exists$ natural number $N$ such that for $n>N$ $$s-\eps<s_n<s+\eps\text{ i.e. }|s-s_n|<\eps$$}
    \qs{}{Is the set ${x-}$axis${\setminus\{\text{Origin}\}}$ a closed set}
    \sol We have to take its complement and check whether that set is a open set i.e. if it is a union of open balls
    \nt{We will do topology in Normed Linear Space  (Mainly $\bbR^n$ and occasionally $\bbC^n$)using the language of Metric Space}
    \clm{Topology}{}{Topology is cool}
    \ex{Open Set and Close Set}{
    	\begin{tabular}{rl}
    		Open Set:   & $\bullet$ $\phi$                                              \\
    		            & $\bullet$ $\bigcup\limits_{x\in X}B_r(x)$ (Any $r>0$ will do) \\[3mm]
    		            & $\bullet$ $B_r(x)$ is open                                    \\
    		Closed Set: & $\bullet$ $X,\ \phi$                                          \\
    		            & $\bullet$ $\overline{B_r(x)}$                                 \\
    		            & $x-$axis $\cup$ $y-$axis
    	\end{tabular}}
    \thm{}{If $x\in$ open set $V$ then $\exists$ $\delta>0$ such that $B_{\delta}(x)\subset V$}
    \begin{myproof}By openness of $V$, $x\in B_r(u)\subset V$
    	\begin{center}
    		\begin{tikzpicture}
    			\draw[red] (0,0) circle [x radius=3.5cm, y radius=2cm] ;
    			\draw (3,1.6) node[red]{$V$};
    			\draw [blue] (1,0) circle (1.45cm) ;
    			\filldraw[blue] (1,0) circle (1pt) node[anchor=north]{$u$};
    			\draw (2.9,0.4) node[blue]{$B_r(u)$};
    			\draw [green!40!black] (1.7,0) circle (0.5cm) node [yshift=0.7cm]{$B_{\delta}(x)$} ;
    			\filldraw[green!40!black] (1.7,0) circle (1pt) node[anchor=west]{$x$};
    		\end{tikzpicture}
    	\end{center}
    
    	Given $x\in B_r(u)\subset V$, we want $\delta>0$ such that $x\in B_{\delta} (x)\subset B_r(u)\subset V$. Let $d=d(u,x)$. Choose $\delta $ such that $d+\delta<r$ (e.g. $\delta<\frac{r-d}{2}$)
    
    	If $y\in B_{\delta}(x)$ we will be done by showing that $d(u,y)<r$ but $$d(u,y)\leq d(u,x)+d(x,y)<d+\delta<r$$
    \end{myproof}
    
    \cor{}{By the result of the proof, we can then show...}
    \mlenma{}{Suppose $\vec{v_1}, \dots, \vec{v_n} \in \RR[n]$ is subspace of $\RR^n$.}
    \mprop{}{$1 + 1 = 2$.}
    
    \section{Random}
    \dfn{Normed Linear Space and Norm $\boldsymbol{\|\cdot\|}$}{Let $V$ be a vector space over $\bbR$ (or $\bbC$). A norm on $V$ is function $\|\cdot\|\ V\to \bbR_{\geq 0}$ satisfying \begin{enumerate}[label=\bfseries\tiny\protect\circled{\small\arabic*}]
    		\item \label{n:1}$\|x\|=0 \iff x=0$ $\forall$ $x\in V$
    		\item \label{n:2}	$\|\lambda x\|=|\lambda|\|x\|$ $\forall$ $\lambda\in\bbR$(or $\bbC$), $x\in V$
    		\item \label{n:3} $\|x+y\| \leq \|x\|+\|y\|$ $\forall$ $x,y\in V$ (Triangle Inequality/Subadditivity)
    	\end{enumerate}And $V$ is called a normed linear space.
    
    	$\bullet $ Same definition works with $V$ a vector space over $\bbC$ (again $\|\cdot\|\to\bbR_{\geq 0}$) where \ref{n:2} becomes $\|\lambda x\|=|\lambda|\|x\|$ $\forall$ $\lambda\in\bbC$, $x\in V$, where for $\lambda=a+ib$, $|\lambda|=\sqrt{a^2+b^2}$ }
    
    
    \ex{$\bs{p-}$Norm}{\label{pnorm}$V={\bbR}^m$, $p\in\bbR_{\geq 0}$. Define for $x=(x_1,x_2,\cdots,x_m)\in\bbR^m$ $$\|x\|_p=\Big(|x_1|^p+|x_2|^p+\cdots+|x_m|^p\Big)^{\frac1p}$$(In school $p=2$)}
    \textbf{Special Case $\bs{p=1}$}: $\|x\|_1=|x_1|+|x_2|+\cdots+|x_m|$ is clearly a norm by usual triangle inequality. \par
    \textbf{Special Case $\bs{p\to\infty\ (\bbR^m$ with $\|\cdot\|_{\infty})}$}: $\|x\|_{\infty}=\max\{|x_1|,|x_2|,\cdots,|x_m|\}$\\
    For $m=1$ these $p-$norms are nothing but $|x|$.
    Now exercise
    \qs{}{\label{exs1}Prove that triangle inequality is true if $p\geq 1$ for $p-$norms. (What goes wrong for $p<1$ ?)}
    \sol{\textbf{For Property \ref{n:3} for norm-2}	\subsubsection*{\textbf{When field is $\bbR:$}} We have to show\begin{align*}
    		         & \sum_i(x_i+y_i)^2\leq \left(\sqrt{\sum_ix_i^2} +\sqrt{\sum_iy_i^2}\right)^2                                       \\
    		\implies & \sum_i (x_i^2+2x_iy_i+y_i^2)\leq \sum_ix_i^2+2\sqrt{\left[\sum_ix_i^2\right]\left[\sum_iy_i^2\right]}+\sum_iy_i^2 \\
    		\implies & \left[\sum_ix_iy_i\right]^2\leq \left[\sum_ix_i^2\right]\left[\sum_iy_i^2\right]
    	\end{align*}So in other words prove $\langle x,y\rangle^2 \leq \langle x,x\rangle\langle y,y\rangle$ where
    	$$\langle x,y\rangle =\sum\limits_i x_iy_i$$
    
    	\begin{note}
    		\begin{itemize}
    			\item $\|x\|^2=\langle x,x\rangle$
    			\item $\langle x,y\rangle=\langle y,x\rangle$
    			\item $\langle \cdot,\cdot\rangle$ is $\bbR-$linear in each slot i.e. \begin{align*}
    				      \langle rx+x',y\rangle=r\langle x,y\rangle+\langle x',y\rangle	\text{ and similarly for second slot}
    			      \end{align*}Here in $\langle x,y\rangle$ $x$ is in first slot and $y$ is in second slot.
    		\end{itemize}
    	\end{note}Now the statement is just the Cauchy-Schwartz Inequality. For proof $$\langle x,y\rangle^2\leq \langle x,x\rangle\langle y,y\rangle $$ expand everything of $\langle x-\lambda y,x-\lambda y\rangle$ which is going to give a quadratic equation in variable $\lambda $ \begin{align*}
    		\langle x-\lambda y,x-\lambda y\rangle & =\langle x,x-\lambda y\rangle-\lambda\langle y,x-\lambda y\rangle                                       \\
    		                                       & =\langle x ,x\rangle -\lambda\langle x,y\rangle -\lambda\langle y,x\rangle +\lambda^2\langle y,y\rangle \\
    		                                       & =\langle x,x\rangle -2\lambda\langle x,y\rangle+\lambda^2\langle y,y\rangle
    	\end{align*}Now unless $x=\lambda y$ we have $\langle x-\lambda y,x-\lambda y\rangle>0$ Hence the quadratic equation has no root therefore the discriminant is greater than zero.
    
    	\subsubsection*{\textbf{When field is $\bbC:$}}Modify the definition by $$\langle x,y\rangle=\sum_i\overline{x_i}y_i$$Then we still have $\langle x,x\rangle\geq 0$}
     \pagebreak
    
    % ================ %
    % == References == %
    % ================ %
    \bibliographystyle{alpha}
    \bibliography{sample}
    
\end{document}