\documentclass[12pt]{report}

\usepackage{classTools}
\usepackage{ialib}
\input{stats-import}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{pgfplots}

\input{preamble}
\input{macros}
\input{letterfonts}

% TODO THINGS TO UPDATE
\pgfplotsset{compat=1.10}

\usepgfplotslibrary{fillbetween}

\hfuzz=10.0pt
\usepackage[parfill,indent]{parskip} % Removes paragraph indentation and adds space between paragraphsimage.png

\pgfmathdeclarefunction{gauss}{3}{%
  \pgfmathparse{1/(#3*sqrt(2*pi))*exp(-((#1-#2)^2)/(2*#3^2))}%
}

% TODO END OF THINGS TO UPDATE

%\title{\Huge{Some Class}\\Random Examples}
%\author{\huge{Your Name}}
\title{%
    \huge{Introduction to Data Compression} \\
    \large{\large{A Linear Algebra Approach}}
}
\author{\large{Layan Ansari, Adam Pearl, Iñaki Arango}}
\date{\today}

\begin{document}
    \maketitle
    
    %\begin{abstract}
    %    \blindtext[3]
    %    \Inote{Consider removing abstract}
    %\end{abstract}
    
    \newpage% or \cleardoublepage
    % \pdfbookmark[<level>]{<title>}{<dest>}
    \pdfbookmark[section]{\contentsname}{toc}
    
    % ================== %
    % == Cover Letter == %
    % ================== %
    \vspace*{1cm}
    \centerline{\huge\huge Cover Letter}
    \vspace*{1cm}
    
    First off, we would like give our sincerest thanks to Kai, our project reader, for his initial feedback on our project proposal and draft, as well as to our peer reviewers (Cole Salvador, Eric Wang, Conan Lu, Connor Yu, Raglan Ward, Ron Nachum, Richard Calvo, Peter Pich, Brave Mugisha) for their thoughtful and insightful advice on how to improve the draft. Some of the main takeaways from the feedback we received was to (1) fill in the missing parts between our sections (i.e. fill in the gaps for the reader so the flow of logic presented in the paper could be fluid and comprehensible), (2) give more detailed explanations and examples for the sections specifically regarding linear algebra and the mathematical tools used to interpret our topic, and (3) add more macro-scale information when necessary, like in the section on Principal Component Analysis, with the aim of setting up a foundation for explaining the math and the motivation behind the calculations and proofs we are trying to show.
    
    In regards to the first point of feedback, we addressed this simply by filling in the missing parts, as well as doing some restructuring of the sections. Were we presented with more time or a longer page count, we may have provided some more sections in-between to further lay the foundations for the mathematical concepts at hand. Regarding the second point of feedback, we made sure to add additional examples for section 4, as well as figures to aid the reader's understanding of the definitions, theorems, and proofs. Lastly, with regards to the third point of feedback, we filled in section 4 with some additional background info and explanation of PCA, and also added a secion (section 5) on the bigger-picture ``uses beyond compression.'' The work was divided such that Iñaki did a lot of the Latex formatting (including the beautiful diagrams and definition/theorem tables) as well as the introduction and incorporation of definitions and theorems, the original draft of some of the proofs were written by Adam and then edited by all three members, and then Layan worked on writing-heavy portions as well as incorporating edits/feedback for the proofs.  

    Potentially the most challenging aspects of figuring out the scope and structure of our paper and attending to the feedback and was first to narrow our topic down enough so that we were not taking on too much (we started off a bit too broad and had to narrow down as we incorporated feedback), and also to simply understand what was actually happening in Principal Component Analysis (for this, we relied on many academic articles and textbook explanations). Finally, we struggled a bit with some of the technicalities for the proof regarding repression of the covariance matrix. However, most of the rough patches were smoothed out through discussion and some back-and-forth about the theorems we were trying to prove. Were we to have more time, we might have refined the proofs a bit or made them a bit longer for clarification of some in-between steps. Additionally, we may have kept one of the sections originally in our draft regarding the ethical, social, and cultural conversations surrounding the work of handwriting analysis, particularly as it relates to and Anglo-centric bias (most studies and software are tailored to the English language/script). On behalf of all three of us, we would like to thank the course staff of Math 22a for an amazing semester--we look forward to continuing more math courses at Harvard!
    
    
    \pagebreak
    
    % ======================= %
    % == Table of Contents == %
    % ======================= %
    \tableofcontents
    
    \pagebreak

    % ============== %
    % == Contents == %
    % ============== % 
    \chapter{Introduction}
        Data flows everywhere, it is in every product that we interact with daily. More than 2.5 quintillion bytes are created every day (that is 2.5 followed by 18 zeros) \cite{alexaqz_2015}. For the next 5 years data is expected to grow by 40\% every year \cite{marr_2022}.

        We need, and will continue to need, more powerful and faster computers capable of processing this increasing amount of information, and better storage systems to safekeep it.

        There has been a lot of improvement hardware-wise in the last couple of decades, which increased the density of our storage systems. However, what if we could improve our information storage density through other, non-physical, means? What if we could store the same amount of ``information'' but with fewer ``bits''? This would allow us to work in parallel with scientists researching physical systems.
        
        This is where data compression comes in. It allows us to store the same amount of ``information'' in less bits, digits, characters, or whatever the most basic unit of storage is in our medium of choice. An early example of compression work is the Morse Code, which was invented in 1838 for use in the telegraph industry. Telegraph bandwidth was scarce, so shorter code words were assigned for common letters in the English alphabet, such as ``e'' and ``t'', that would be sent more often \cite{Wolfram2002}.

        Compression can also be useful for analysis and recognition of data patterns. Suppose that given a telgraph system, we had decided to make a picture for every single letter in the English alphabet, and instead of sending Morse code over a telegraph, we had sent the all the pixel information for the letter images every time we wanted to transmit a sentence across the wire. Not only would that have taken so much longer and wasted bandwidth (related to compression), but it would also have been way harder to recognize and would have been more error prone (people's caligraphy is different from one another). One would have to record all the pixel information for every letter, reconstruct it, and then decide on which letter it is: a computationally hard and intensive task for both, a computer and a human.

        Morse code, an example of compression, shows how crucial it is to find reliable ways of storing and sharing our information in a way that minimizes space used and maximizes reliability.
    
    \chapter{Mathematical Background}
        There are various tools at our disposal that we can use to study and perform data compression and analysis. Many of the common techniques that are used today are the outcome of collaboration between scientists and engineers focused on different branches of Math, such as Probability, Statistics, Information Theory, and Linear Algebra.
        
        While we have focused on Linear Algebra this semester, it is necessary to know some introductory concepts from these other branches to understand basic data compression. In this section we will introduce these branches and explain some of their basic concepts.
        
        \section{Probability \& Statistics}
            Probability is the study of the the likelihood of events, independently and given the occurrence of other events (e.g., given that it is cloudy, what is the probability that it will rain today?). Statistics is the discipline that concerns the collection, organization, analysis, interpretation, and presentation of data \cite{wiki:Statistics}.

            \subsection{Sample Space}
                The most basic concept in probability is that of the \textbf{sample space} and \textbf{events}.
                \dfn{Sample Space}{
                    The \textbf{sample space} is the set of all possible outcomes that an experiment can have.
                }
                \ex{Flipping a coin}{
                    If we flip a coin then the sample space is $S = \{\text{Heads}, \text{Tails}\}$.
                }
                \ex{Rolling a die}{
                    If we roll a single 6-sided die the sample space is $S = \{1, 2, 3, 4, 5, 6\}$.
                }
                
            \subsection{Events}
                \dfn{Event}{
                    \textbf{Events} are then mathematically defined as a subset of the sample space.
                }
                \ex{Rolling a die}{
                    For example, in the 6-sided die example let $M$ be the event we roll a 4 or greater. Then $M = \{4, 5, 6\} \subseteq S$.
                    
                    \smallskip In this experiment we state that we are rolling the die once, so we cannot make any statements about what happens if you first roll something, and the something else. In that case, once would have to redefine the experiment to say that each die roll has possibilities $R = \{1, 2, 3, 4, 5, 6\}$ and the total experiment with $n$ roles has sample space $S = R^n$.
                }

            \subsection{Probability}
                \dfn{Probability}{
                    The \textbf{probability} of an event (denote $P(M)$ for an event $M$) is the measure of how likely the event is to occur as the outcome of a random experiment. It is the sum of the probabilities of each element of the set being the outcome of the performed experiment.
                }

                Probability is measured out of 1, which means that events certain to occur have a probability of 1 and events that will never occur have a probability of 0.

                Since the sample space contains all the possible outcomes for the experiment, it is guaranteed that one the sample space's elements will be the outcome of the experiment, so \[P(S) = 1\].
            
            \subsection{Random Variables}
                \dfn{Random Variables}{
                    \textbf{Random variables} (also r.v.'s) are functions that map the outcome of a random experiment (in the sample space) to a measurable quantity (e.g., real number) or a mathematical object (e.g., matrix).
                }

                When the experiment is performed, and the outcome is mapped to a value, we say the random variable ``crystallizes'' to that value. The set of all values a random variable crystallizes to is called the support, denoted $R_X$ for a r.v. $X$. A specified random variable crystallizing to a certain value is an event.

                We normally denote random variables with capital letters and their crystallized values with lower case letters.

                \ex{Flipping a coin}{
                    Let us flip a fair coin with equal probabilities of landing heads or tails. The sample space is then $S = \{H, T\}$ with $P(\{H\}) = P(\{T\}) = 0.5$.

                    \smallskip Let $X$ be a random variable that crystallizes to -1 if the coin lands Heads and to 1 if the coin lands Tails. Then $(X = -1)$ and $(X = 1)$ are events and we write $P(X = -1) = P(\{H\})$ and $P(X = 1) = P(\{T\})$.

                    \smallskip If we where to write the probability function for $X$, where $x$ is the crystallized value that $X$ takes, we would get
                        \[P(X = x) = \begin{cases} 0.5 & \text{if } x = -1 \\ 0.5 & \text{if } x = 1 \end{cases}\]
                }

            \subsection{Expectation}
                \dfn{Expectation}{
                    The \textbf{expectation} (also called the mean) of a random variable $X$, denoted $\IAexpected(X)$, is the weighted average of the values that $X$ can take. The weight for a value $x$ is determined by the probability of $X$ crystallizing to that value.
                    
                    \smallskip We also write the variable with a bar on top to represent its mean. Its formula is
                        \[\IAmean{X} = \IAexpected(X) = \sum_{x \in R_X} x \cdot P(X = x)\]
                }

                \ex{Rolling a die}{
                    Expanding on the previous example of rolling a 6-sided die once. Let $X$ be a random variable that crystallizes to the number rolled. Then $R_X = \{1, 2, 3, 4, 5, 6\}$ and $P(X = x) = \frac{1}{6}$ for all $x \in R_X$, since each number has the same probability of being rolled.

                    \smallskip The expected value of $X$ is then
                        \[\IAexpected(X) = \sum_{x \in R_X} x \cdot P(X = x) = \frac{1}{6} \cdot 1 + \frac{1}{6} \cdot 2 + \frac{1}{6} \cdot 3 + \frac{1}{6} \cdot 4 + \frac{1}{6} \cdot 5 + \frac{1}{6} \cdot 6 = \frac{21}{6} = 3.5\]
                }
            
            \subsection{Variance}
                \dfn{Variance}{
                    \textbf{Variance} is a measure of spread. It measures how far the values in the support of an r.v. are to the mean. For a r.v. $X$ we denote its variance as $\var(X)$. The formula for variance is
                        \[\var(X) = \IAexpected[(X - \IAmean{X})^2] = \IAexpected(X^2) - [\IAexpected(X)]^2\]
                }

                Below is a graphical comparison of between the probability function of a low variance r.v. $X$ and the probability function of a high variance r.v. $Y$. The horizontal axis indicates a value in the support of the random variable and the vertical axis indicates the probability of the random variable taking that value.

                \begin{figure}[H]
                    \centering
                    \begin{minipage}{0.45\textwidth}
                        \centering
                        \begin{tikzpicture}
                            \begin{axis}[
                                width=3.25in,
                                height=2.25in,
                                xmin=-1,
                                xmax=5,
                                ymin=-0.1,
                                ymax=0.5,
                                axis lines=middle,
                                xlabel={$x$},
                                ylabel={$P(X=x)$},
                                samples=50]
                                \addplot [name path=f, very thick,blue!75!black] {gauss(x, 2.5, 1)};
                                \path[name path=axis] (axis cs:-1,0) -- (axis cs:5,0);
                                \addplot [
                                    thick,
                                    color=blue,
                                    fill=blue, 
                                    fill opacity=0.1
                                ]
                                fill between[
                                    of=f and axis,
                                ];
                            \end{axis}
                        \end{tikzpicture}
                        Low Variance
                        \label{fig:low_variance}
                    \end{minipage}
                    \begin{minipage}{0.45\textwidth}
                        \centering
                        \begin{tikzpicture}
                            \begin{axis}[
                                width=3.25in,
                                height=2.25in,
                                xmin=-1,
                                xmax=5,
                                ymin=-0.1,
                                ymax=0.5,
                                axis lines=middle,
                                xlabel={$y$},
                                ylabel={$P(Y=y)$},
                                samples=50]
                                \addplot [name path=f, very thick,blue!75!black] {gauss(x, 2.5, 2)};
                                \path[name path=axis] (axis cs:-1,0) -- (axis cs:5,0);
                                \addplot [
                                    thick,
                                    color=blue,
                                    fill=blue, 
                                    fill opacity=0.1
                                ]
                                fill between[
                                    of=f and axis,
                                ];
                            \end{axis}
                        \end{tikzpicture}
                        High Variance
                        \label{fig:high_variance}
                    \end{minipage}%
                \end{figure}

                % \dfn{Standard Deviation}{
                %     Its cousin, \textbf{standard deviation}, is defined as $\sigma = \sqrt{\var(X)}$, and is used because it is sometimes more practical to express the spread in terms of standard deviation.
                % }
                % \Knote{Why? Might be helpful to explain it a little bit more to aid understanding in the rest of the paper}

            
            \subsection{Covariance}
                \dfn{Covariance}{
                    While the variance is the measure of spread for a single variables, the \textbf{covariance} examines the directional relationship between two variables \cite{Rice2007}. Its formula is
                        \[\Cov(X, Y) = \IAexpected[(X - \IAmean{X})(Y - \IAmean{Y})] = \IAexpected(X \cdot Y) - \IAexpected(X)\IAexpected(Y)\]
                }

                If greater values of one variable correspond to greater values of the other variable covariance is positive. If greater values of one variable correspond to lesser values of the other variable, covariance is negative.

                Below is a graphical comparison of what negative, zero and positive covariance looks like between two random variables. Each point encodes one instance of the two variables crystallized together.

                \begin{figure}[H]
                    \centering
                    \begin{minipage}{0.3\textwidth}
                        \centering
                        \begin{tikzpicture}
                            \begin{axis}[
                                width=2.25in,
                                height=2.25in,
                                xmin=-1,
                                xmax=5,
                                ymin=-1,
                                ymax=5,
                                axis lines=middle,
                                xlabel={$X$},
                                ylabel={$Y$}]
                                \addplot[mark=*] coordinates {(1,4)} node(A){};
                                \addplot[mark=*] coordinates {(2,3)};
                                \addplot[mark=*] coordinates {(2,2)};
                                \addplot[mark=*] coordinates {(3,1)};
                                \addplot[mark=*] coordinates {(4,1)} node(B){};
                                \addplot[mark=*] coordinates {(4,2)};
                                \addplot[mark=*] coordinates {(3,3)};
                                \addplot[mark=*] coordinates {(2,4)};
                                \node (C)[very thick,draw=blue!75!black,ellipse,rotate fit=40,fit=(A) (B),fill=blue,opacity=0.1] {};
                            \end{axis}
                        \end{tikzpicture}
                        $\cov(X, Y) < 0$
                        \label{fig:negative_cov}
                    \end{minipage}
                    \begin{minipage}{0.3\textwidth}
                        \centering
                        \begin{tikzpicture}
                            \begin{axis}[
                                width=2.25in,
                                height=2.25in,
                                xmin=-1,
                                xmax=5,
                                ymin=-1,
                                ymax=5,
                                axis lines=middle,
                                xlabel={$X$},
                                ylabel={$Y$}]
                                \addplot[mark=*] coordinates {(1.5,2.5)};
                                \addplot[mark=*] coordinates {(2,3)};
                                \addplot[mark=*] coordinates {(3,2)} node(B){};
                                \addplot[mark=*] coordinates {(2,1)};
                                \addplot[mark=*] coordinates {(2,2)};
                                \addplot[mark=*] coordinates {(2,3)};
                                \addplot[mark=*] coordinates {(1,2)} node(A){};
                                \addplot[mark=*] coordinates {(2.75,1.75)};
                                \node (C)[very thick,draw=blue!75!black,ellipse,rotate fit=40,fit=(A) (B),fill=blue,opacity=0.1] {};
                            \end{axis}
                        \end{tikzpicture}
                        $\cov(X, Y) = 0$
                        \label{fig:zero_cov}
                    \end{minipage}
                    \begin{minipage}{.3\textwidth}
                        \centering
                        \begin{tikzpicture}
                            \begin{axis}[
                                width=2.25in,
                                height=2.25in,
                                xmin=-1,
                                xmax=5,
                                ymin=-1,
                                ymax=5,
                                axis lines=middle,
                                xlabel={$X$},
                                ylabel={$Y$}]
                                \addplot[mark=*] coordinates {(1,1)} node(A){};
                                \addplot[mark=*] coordinates {(2,2)};
                                \addplot[mark=*] coordinates {(2,3)};
                                \addplot[mark=*] coordinates {(3,4)};
                                \addplot[mark=*] coordinates {(4,4)} node(B){};
                                \addplot[mark=*] coordinates {(4,3)};
                                \addplot[mark=*] coordinates {(3,2)};
                                \addplot[mark=*] coordinates {(2,1)};
                                \node (C)[very thick,draw=blue!75!black,ellipse,rotate fit=-40,fit=(A) (B),fill=blue,opacity=0.1] {};
                            \end{axis}
                        \end{tikzpicture}
                        $\cov(X, Y) > 0$
                        \label{fig:positive_cov}
                    \end{minipage}%
                \end{figure}
                
                If covariance is 0, we say that variables are ``uncorrelated'', and we interpret it as there being no linear relationship between the two.
    
    \chapter{Start of the Story: What do we want to compress?}
        \section{Handwriting Recognition}
            
            One of the oldest and most accessible methods of communication within a given language is writing by hand. Although in recent years there has been a large effort to make society paper-free, computers still process paper-based documents from all over the world every day in order to handle, retrieve, and store information. An efficiency problem arises, however, from the fact that it requires a lot of time and money to manually enter the data from these documents into computers \cite{Bortolozzi2005}. A solution to this efficiency problem is required, as these documents may continue to need processing for the foreseeable future due to a number of reasons, including historical record keeping. Up until recently, most of these were handwritten or on print paper. Examples include doctors' prescriptions, government documents, and immigration papers.
        
            Thus, handwriting recognition is the task of transcribing handwritten data into a digital format, and this task obviously benefits from data compression. The goal is to electronically process handwritten data, and to do so in a manner that achieves the same accuracy as humans \cite{Gunter2005}.
            
            Handwriting can essentially be categorized as either print or cursive script, and for both categories, recognition accuracy is the main problem because of the similar strokes and shapes of some letters. Factors such as illegibility of handwriting can yield inaccurate recognition of a letter by the software \cite{Holzinger2012}. Finally, a complicating problem that makes recognition notably difficult (especially for cursive) is the absence of obvious character boundaries; while print handwriting often has gaps and spaces between each letter, this is most often not the case for cursive writing, and so it is hard to know the start and stop of recognition per character \cite{Puigcerver2017}.
            
            This is where the data compression/dimension reduction and eigenface technique comes into play! In essence, if we have a large data set that consists of thousands or even millions of images of words (probably limited to one language), we want to find a way to recognize patterns in these images. From these patterns, we can then determine which ones have the most ``importance'' and attempt to express these images as a weighted combination of these most important patterns (and thus in a lower dimension).
        
    
    \chapter{Principal Component Analysis}
        \section{Extracting ``high information'' regions in the data space}
            In this chapter we will see how we can use the knowledge from Chapter 2 to find efficient representations for arbitrary data, and apply that to the challenge introduced in Chapter 3.

            Data in its raw or most simple representation is likely to the potential to store many more values than we actually care about. For example, in the case of character recognition, lets say that the dataset that we have for each handwritten character consists a 28x28 pixel grayscale images.

            \begin{figure}[H]
                \centering
                \includegraphics[width=0.7\textwidth]{mnist-handwriting-dataset.jpg}
                \caption{\label{fig:mnist-handwriting-dataset} Samples of the numbers 5, 0, 4, and 1 from the MNIST handwriting dataset.}
            \end{figure}

            Theoretically, with this data space, we can encode signifcantly more than the letters and digits in the English alphabet. An all black or all white image exists in this data space but does not represent any of the glyphs that we care about.

            The aim of data compression is to find a way of reducing the size of our data representation while preserving the ability to reconstruct the information in the original uncompressed data space at will.

            In the case of the alphabet and humans, it is quite straight forward. A person can read a character, store it in the computer as a numerical value (0-9 for `0'-`9' and 10-35 for `a'-`z'), and then when we want an image again, a human can write the digit out. But this techinque is very restricted. It can only be applied to the digit the characters the human is able to remember and that we had previously encoded as a numeric value. If the person sees a new digit they do not recognize, there is no way of storing this information using this compression format.

            We need to establish a system that is able to solve this problem, that can be performed by computers without human intervention, and that can encode more than a predefined set of discrete values (ideally a continuous spectrum of variables).

            A common techinque that has these capabilities is calles Principal Component Analysis (PCA). At a high level, it consists of finding patterns in the data (called components) and creating a system that allows us to express a datapoint in a our original data space in terms of how much of each component appears in that point.

            \ex{Compressing image of plaid patterns}{
                In this example, we will look at a set of images that consists of plaid patterns and see how PCA can be useful in reducing the amount of data it takes to store these.
            }

            

            \begin{figure}[H]
                \centering
                \begin{minipage}{0.2\textwidth}
                    \centering
                    \IApixelimage[1.25in]{
                        {3,5,5,5,5,5,5,3},
                        {5,7,7,7,7,7,7,5},
                        {5,7,7,7,7,7,7,5},
                        {5,7,7,7,7,7,7,5},
                        {5,7,7,7,7,7,7,5},
                        {5,7,7,7,7,7,7,5},
                        {5,7,7,7,7,7,7,5},
                        {3,5,5,5,5,5,5,3}%
                    }
                    Image 1
                    \label{fig:plaid_img_1}
                \end{minipage}
                \begin{minipage}{0.2\textwidth}
                    \centering
                    \IApixelimage[1.25in]{
                        {7,5,7,7,7,7,5,7},
                        {5,3,5,5,5,5,3,5},
                        {7,5,7,7,7,7,5,7},
                        {7,5,7,7,7,7,5,7},
                        {7,5,7,7,7,7,5,7},
                        {7,5,7,7,7,7,5,7},
                        {5,3,5,5,5,5,3,5},
                        {7,5,7,7,7,7,5,7}%
                    }
                    Image 2
                    \label{fig:plaid_img_2}
                \end{minipage}
                \begin{minipage}{0.2\textwidth}
                    \centering
                    \IApixelimage[1.25in]{
                        {7,7,5,7,5,7,7,7},
                        {7,7,5,7,5,7,7,7},
                        {5,5,3,5,3,5,5,5},
                        {7,7,5,7,5,7,7,7},
                        {5,5,3,5,3,5,5,5},
                        {7,7,5,7,5,7,7,7},
                        {7,7,5,7,5,7,7,7},
                        {7,7,5,7,5,7,7,7}%
                    }
                    Image 3
                    \label{fig:plaid_img_3}
                \end{minipage}
                \begin{minipage}{0.2\textwidth}
                    \centering
                    \IApixelimage[1.25in]{
                        {7,7,7,5,5,7,7,7},
                        {7,7,7,5,5,7,7,7},
                        {7,7,7,5,5,7,7,7},
                        {5,5,5,3,3,5,5,5},
                        {7,7,7,5,5,7,7,7},
                        {5,5,5,3,3,5,5,5},
                        {7,7,7,5,5,7,7,7},
                        {7,7,7,5,5,7,7,7}%
                    }
                    Image 4
                    \label{fig:plaid_img_4}
                \end{minipage}%
            \end{figure}

            The standard representation of this data would be a vector of $8 \times 8 = 64$ dimensions, where each dimension can take one of three values: 0 (black), 1 (gray) or 2 (white). This data space would have $3^64 \approx 3.43 \cdot 10^{30}$ elements but most of them would not represent plaid patterns, which is what we know our data will always represent.

            \nt{To store one of this datapoints would require storing the pixel value for each dimension, resulting in a storage size of 128 storage units.}

            What if we expressed these datapoints in a more favorable basis? Let the following be our new basis for expressing datapoints.
            
            \begin{figure}[H]
                \centering
                \begin{minipage}{0.1\textwidth}
                    \centering
                    \IApixelimage[0.65in]{
                        {5,7,7,7,7,7,7,7},
                        {5,7,7,7,7,7,7,7},
                        {5,7,7,7,7,7,7,7},
                        {5,7,7,7,7,7,7,7},
                        {5,7,7,7,7,7,7,7},
                        {5,7,7,7,7,7,7,7},
                        {5,7,7,7,7,7,7,7},
                        {5,7,7,7,7,7,7,7}%
                    }
                    Vec. 1
                \end{minipage}
                \begin{minipage}{0.1\textwidth}
                    \centering
                      \IApixelimage[0.65in]{
                        {7,5,7,7,7,7,7,7},
                        {7,5,7,7,7,7,7,7},
                        {7,5,7,7,7,7,7,7},
                        {7,5,7,7,7,7,7,7},
                        {7,5,7,7,7,7,7,7},
                        {7,5,7,7,7,7,7,7},
                        {7,5,7,7,7,7,7,7},
                        {7,5,7,7,7,7,7,7}%
                    }
                    Vec. 2
                \end{minipage}
                \begin{minipage}{0.1\textwidth}
                    \centering
                    \IApixelimage[0.65in]{
                        {7,7,5,7,7,7,7,7},
                        {7,7,5,7,7,7,7,7},
                        {7,7,5,7,7,7,7,7},
                        {7,7,5,7,7,7,7,7},
                        {7,7,5,7,7,7,7,7},
                        {7,7,5,7,7,7,7,7},
                        {7,7,5,7,7,7,7,7},
                        {7,7,5,7,7,7,7,7}%
                    }
                    Vec. 3
                \end{minipage}
                \begin{minipage}{0.1\textwidth}
                    \centering
                    \IApixelimage[0.65in]{
                        {7,7,7,5,7,7,7,7},
                        {7,7,7,5,7,7,7,7},
                        {7,7,7,5,7,7,7,7},
                        {7,7,7,5,7,7,7,7},
                        {7,7,7,5,7,7,7,7},
                        {7,7,7,5,7,7,7,7},
                        {7,7,7,5,7,7,7,7},
                        {7,7,7,5,7,7,7,7}%
                    }
                    Vec. 4
                \end{minipage}
                \begin{minipage}{0.1\textwidth}
                    \centering
                    \IApixelimage[0.65in]{
                        {7,7,7,7,5,7,7,7},
                        {7,7,7,7,5,7,7,7},
                        {7,7,7,7,5,7,7,7},
                        {7,7,7,7,5,7,7,7},
                        {7,7,7,7,5,7,7,7},
                        {7,7,7,7,5,7,7,7},
                        {7,7,7,7,5,7,7,7},
                        {7,7,7,7,5,7,7,7}%
                    }
                    Vec. 5
                \end{minipage}
                \begin{minipage}{0.1\textwidth}
                    \centering
                    \IApixelimage[0.65in]{
                        {7,7,7,7,7,5,7,7},
                        {7,7,7,7,7,5,7,7},
                        {7,7,7,7,7,5,7,7},
                        {7,7,7,7,7,5,7,7},
                        {7,7,7,7,7,5,7,7},
                        {7,7,7,7,7,5,7,7},
                        {7,7,7,7,7,5,7,7},
                        {7,7,7,7,7,5,7,7}%
                    }
                    Vec. 6
                \end{minipage}
                \begin{minipage}{0.1\textwidth}
                    \centering
                    \IApixelimage[0.65in]{
                        {7,7,7,7,7,7,5,7},
                        {7,7,7,7,7,7,5,7},
                        {7,7,7,7,7,7,5,7},
                        {7,7,7,7,7,7,5,7},
                        {7,7,7,7,7,7,5,7},
                        {7,7,7,7,7,7,5,7},
                        {7,7,7,7,7,7,5,7},
                        {7,7,7,7,7,7,5,7}%
                    }
                    Vec. 7
                \end{minipage}
                \begin{minipage}{0.1\textwidth}
                    \centering
                    \IApixelimage[0.65in]{
                        {7,7,7,7,7,7,7,5},
                        {7,7,7,7,7,7,7,5},
                        {7,7,7,7,7,7,7,5},
                        {7,7,7,7,7,7,7,5},
                        {7,7,7,7,7,7,7,5},
                        {7,7,7,7,7,7,7,5},
                        {7,7,7,7,7,7,7,5},
                        {7,7,7,7,7,7,7,5}%
                    }
                    Vec. 8
                \end{minipage}%
            \end{figure}
            \begin{figure}[H]
                \centering
                \begin{minipage}{0.1\textwidth}
                    \centering
                    \IApixelimage[0.65in]{
                        {5,5,5,5,5,5,5,5},
                        {7,7,7,7,7,7,7,7},
                        {7,7,7,7,7,7,7,7},
                        {7,7,7,7,7,7,7,7},
                        {7,7,7,7,7,7,7,7},
                        {7,7,7,7,7,7,7,7},
                        {7,7,7,7,7,7,7,7},
                        {7,7,7,7,7,7,7,7}%
                    }
                    Vec. 9
                \end{minipage}
                \begin{minipage}{0.1\textwidth}
                    \centering
                    \IApixelimage[0.65in]{
                        {7,7,7,7,7,7,7,7},
                        {5,5,5,5,5,5,5,5},
                        {7,7,7,7,7,7,7,7},
                        {7,7,7,7,7,7,7,7},
                        {7,7,7,7,7,7,7,7},
                        {7,7,7,7,7,7,7,7},
                        {7,7,7,7,7,7,7,7},
                        {7,7,7,7,7,7,7,7}%
                    }
                    Vec. 10
                \end{minipage}
                \begin{minipage}{0.1\textwidth}
                    \centering
                    \IApixelimage[0.65in]{
                        {7,7,7,7,7,7,7,7},
                        {7,7,7,7,7,7,7,7},
                        {5,5,5,5,5,5,5,5},
                        {7,7,7,7,7,7,7,7},
                        {7,7,7,7,7,7,7,7},
                        {7,7,7,7,7,7,7,7},
                        {7,7,7,7,7,7,7,7},
                        {7,7,7,7,7,7,7,7}%
                    }
                    Vec. 11
                \end{minipage}
                \begin{minipage}{0.1\textwidth}
                    \centering
                    \IApixelimage[0.65in]{
                        {7,7,7,7,7,7,7,7},
                        {7,7,7,7,7,7,7,7},
                        {7,7,7,7,7,7,7,7},
                        {5,5,5,5,5,5,5,5},
                        {7,7,7,7,7,7,7,7},
                        {7,7,7,7,7,7,7,7},
                        {7,7,7,7,7,7,7,7},
                        {7,7,7,7,7,7,7,7}%
                    }
                    Vec. 12
                \end{minipage}
                \begin{minipage}{0.1\textwidth}
                    \centering
                    \IApixelimage[0.65in]{
                        {7,7,7,7,7,7,7,7},
                        {7,7,7,7,7,7,7,7},
                        {7,7,7,7,7,7,7,7},
                        {7,7,7,7,7,7,7,7},
                        {5,5,5,5,5,5,5,5},
                        {7,7,7,7,7,7,7,7},
                        {7,7,7,7,7,7,7,7},
                        {7,7,7,7,7,7,7,7}%
                    }
                    Vec. 13
                \end{minipage}
                \begin{minipage}{0.1\textwidth}
                    \centering
                    \IApixelimage[0.65in]{
                        {7,7,7,7,7,7,7,7},
                        {7,7,7,7,7,7,7,7},
                        {7,7,7,7,7,7,7,7},
                        {7,7,7,7,7,7,7,7},
                        {7,7,7,7,7,7,7,7},
                        {5,5,5,5,5,5,5,5},
                        {7,7,7,7,7,7,7,7},
                        {7,7,7,7,7,7,7,7}%
                    }
                    Vec. 14
                \end{minipage}
                \begin{minipage}{0.1\textwidth}
                    \centering
                    \IApixelimage[0.65in]{
                        {7,7,7,7,7,7,7,7},
                        {7,7,7,7,7,7,7,7},
                        {7,7,7,7,7,7,7,7},
                        {7,7,7,7,7,7,7,7},
                        {7,7,7,7,7,7,7,7},
                        {7,7,7,7,7,7,7,7},
                        {5,5,5,5,5,5,5,5},
                        {7,7,7,7,7,7,7,7}%
                    }
                    Vec. 15
                \end{minipage}
                \begin{minipage}{0.1\textwidth}
                    \centering
                    \IApixelimage[0.65in]{
                        {7,7,7,7,7,7,7,7},
                        {7,7,7,7,7,7,7,7},
                        {7,7,7,7,7,7,7,7},
                        {7,7,7,7,7,7,7,7},
                        {7,7,7,7,7,7,7,7},
                        {7,7,7,7,7,7,7,7},
                        {7,7,7,7,7,7,7,7},
                        {5,5,5,5,5,5,5,5}%
                    }
                    Vec. 16
                \end{minipage}%
            \end{figure}
            
            If we express our datapoints as a linear combination of these vectors, then we can recreate any of the images presented above, and any other plaid pattern that we want. Importantly, we represent any pattern by just 16 values, the coefficients for each basis vector.

            With this basis, we can only represent $2^{16} = 65536$ datapoints, but those will be all the plaid pattern combinations that exists within the original dataspace. For example we can obtain the four images above by taking the following linear combinations of the basis vectors:

            \begin{figure}[H]
                \centering
                \begin{minipage}{0.1\textwidth}
                    \centering
                    \IApixelimage[0.65in]{
                        {5,7,7,7,7,7,7,7},
                        {5,7,7,7,7,7,7,7},
                        {5,7,7,7,7,7,7,7},
                        {5,7,7,7,7,7,7,7},
                        {5,7,7,7,7,7,7,7},
                        {5,7,7,7,7,7,7,7},
                        {5,7,7,7,7,7,7,7},
                        {5,7,7,7,7,7,7,7}%
                    }
                    Vec. 1
                \end{minipage}
                \begin{minipage}{0.035\textwidth}
                    \centering
                    \quad+\quad
                \end{minipage}
                \begin{minipage}{0.1\textwidth}
                    \centering
                    \IApixelimage[0.65in]{
                        {7,7,7,7,7,7,7,5},
                        {7,7,7,7,7,7,7,5},
                        {7,7,7,7,7,7,7,5},
                        {7,7,7,7,7,7,7,5},
                        {7,7,7,7,7,7,7,5},
                        {7,7,7,7,7,7,7,5},
                        {7,7,7,7,7,7,7,5},
                        {7,7,7,7,7,7,7,5}%
                    }
                    Vec. 8
                \end{minipage}
                \begin{minipage}{0.035\textwidth}
                    \centering
                    \quad+\quad
                \end{minipage}
                \begin{minipage}{0.1\textwidth}
                    \centering
                    \IApixelimage[0.65in]{
                        {5,5,5,5,5,5,5,5},
                        {7,7,7,7,7,7,7,7},
                        {7,7,7,7,7,7,7,7},
                        {7,7,7,7,7,7,7,7},
                        {7,7,7,7,7,7,7,7},
                        {7,7,7,7,7,7,7,7},
                        {7,7,7,7,7,7,7,7},
                        {7,7,7,7,7,7,7,7}%
                    }
                    Vec. 9
                \end{minipage}
                \begin{minipage}{0.035\textwidth}
                    \centering
                    \quad+\quad
                \end{minipage}
                \begin{minipage}{0.1\textwidth}
                    \centering
                    \IApixelimage[0.65in]{
                        {7,7,7,7,7,7,7,7},
                        {7,7,7,7,7,7,7,7},
                        {7,7,7,7,7,7,7,7},
                        {7,7,7,7,7,7,7,7},
                        {7,7,7,7,7,7,7,7},
                        {7,7,7,7,7,7,7,7},
                        {7,7,7,7,7,7,7,7},
                        {5,5,5,5,5,5,5,5}%
                    }
                    Vec. 16
                \end{minipage}
                \begin{minipage}{0.035\textwidth}
                    \centering
                    \quad=\quad
                \end{minipage}
                \begin{minipage}{0.1\textwidth}
                    \centering
                    \IApixelimage[0.65in]{
                        {3,5,5,5,5,5,5,3},
                        {5,7,7,7,7,7,7,5},
                        {5,7,7,7,7,7,7,5},
                        {5,7,7,7,7,7,7,5},
                        {5,7,7,7,7,7,7,5},
                        {5,7,7,7,7,7,7,5},
                        {5,7,7,7,7,7,7,5},
                        {3,5,5,5,5,5,5,3}%
                    }
                    Image 1
                \end{minipage}%
            \end{figure}

            \begin{figure}[H]
                \centering
                \begin{minipage}{0.1\textwidth}
                    \centering
                    \IApixelimage[0.65in]{
                        {7,5,7,7,7,7,7,7},
                        {7,5,7,7,7,7,7,7},
                        {7,5,7,7,7,7,7,7},
                        {7,5,7,7,7,7,7,7},
                        {7,5,7,7,7,7,7,7},
                        {7,5,7,7,7,7,7,7},
                        {7,5,7,7,7,7,7,7},
                        {7,5,7,7,7,7,7,7}%
                    }
                    Vec. 2
                \end{minipage}
                \begin{minipage}{0.035\textwidth}
                    \centering
                    \quad+\quad
                \end{minipage}
                \begin{minipage}{0.1\textwidth}
                    \centering
                    \IApixelimage[0.65in]{
                        {7,7,7,7,7,7,5,7},
                        {7,7,7,7,7,7,5,7},
                        {7,7,7,7,7,7,5,7},
                        {7,7,7,7,7,7,5,7},
                        {7,7,7,7,7,7,5,7},
                        {7,7,7,7,7,7,5,7},
                        {7,7,7,7,7,7,5,7},
                        {7,7,7,7,7,7,5,7}%
                    }
                    Vec. 7
                \end{minipage}
                \begin{minipage}{0.035\textwidth}
                    \centering
                    \quad+\quad
                \end{minipage}
                \begin{minipage}{0.1\textwidth}
                    \centering
                    \IApixelimage[0.65in]{
                        {7,7,7,7,7,7,7,7},
                        {5,5,5,5,5,5,5,5},
                        {7,7,7,7,7,7,7,7},
                        {7,7,7,7,7,7,7,7},
                        {7,7,7,7,7,7,7,7},
                        {7,7,7,7,7,7,7,7},
                        {7,7,7,7,7,7,7,7},
                        {7,7,7,7,7,7,7,7}%
                    }
                    Vec. 10
                \end{minipage}
                \begin{minipage}{0.035\textwidth}
                    \centering
                    \quad+\quad
                \end{minipage}
                \begin{minipage}{0.1\textwidth}
                    \centering
                    \IApixelimage[0.65in]{
                        {7,7,7,7,7,7,7,7},
                        {7,7,7,7,7,7,7,7},
                        {7,7,7,7,7,7,7,7},
                        {7,7,7,7,7,7,7,7},
                        {7,7,7,7,7,7,7,7},
                        {7,7,7,7,7,7,7,7},
                        {5,5,5,5,5,5,5,5},
                        {7,7,7,7,7,7,7,7}%
                    }
                    Vec. 15
                \end{minipage}
                \begin{minipage}{0.035\textwidth}
                    \centering
                    \quad=\quad
                \end{minipage}
                \begin{minipage}{0.1\textwidth}
                    \centering
                    \IApixelimage[0.65in]{
                        {7,5,7,7,7,7,5,7},
                        {5,3,5,5,5,5,3,5},
                        {7,5,7,7,7,7,5,7},
                        {7,5,7,7,7,7,5,7},
                        {7,5,7,7,7,7,5,7},
                        {7,5,7,7,7,7,5,7},
                        {5,3,5,5,5,5,3,5},
                        {7,5,7,7,7,7,5,7}%
                    }
                    Image 2
                \end{minipage}%
            \end{figure}

            \begin{figure}[H]
                \centering
                \begin{minipage}{0.1\textwidth}
                    \centering
                    \IApixelimage[0.65in]{
                        {7,7,5,7,7,7,7,7},
                        {7,7,5,7,7,7,7,7},
                        {7,7,5,7,7,7,7,7},
                        {7,7,5,7,7,7,7,7},
                        {7,7,5,7,7,7,7,7},
                        {7,7,5,7,7,7,7,7},
                        {7,7,5,7,7,7,7,7},
                        {7,7,5,7,7,7,7,7}%
                    }
                    Vec. 3
                \end{minipage}
                \begin{minipage}{0.035\textwidth}
                    \centering
                    \quad+\quad
                \end{minipage}
                \begin{minipage}{0.1\textwidth}
                    \centering
                    \IApixelimage[0.65in]{
                        {7,7,7,7,5,7,7,7},
                        {7,7,7,7,5,7,7,7},
                        {7,7,7,7,5,7,7,7},
                        {7,7,7,7,5,7,7,7},
                        {7,7,7,7,5,7,7,7},
                        {7,7,7,7,5,7,7,7},
                        {7,7,7,7,5,7,7,7},
                        {7,7,7,7,5,7,7,7}%
                    }
                    Vec. 5
                \end{minipage}
                \begin{minipage}{0.035\textwidth}
                    \centering
                    \quad+\quad
                \end{minipage}
                \begin{minipage}{0.1\textwidth}
                    \centering
                    \IApixelimage[0.65in]{
                        {7,7,7,7,7,7,7,7},
                        {7,7,7,7,7,7,7,7},
                        {5,5,5,5,5,5,5,5},
                        {7,7,7,7,7,7,7,7},
                        {7,7,7,7,7,7,7,7},
                        {7,7,7,7,7,7,7,7},
                        {7,7,7,7,7,7,7,7},
                        {7,7,7,7,7,7,7,7}%
                    }
                    Vec. 11
                \end{minipage}
                \begin{minipage}{0.035\textwidth}
                    \centering
                    \quad+\quad
                \end{minipage}
                \begin{minipage}{0.1\textwidth}
                    \centering
                    \IApixelimage[0.65in]{
                        {7,7,7,7,7,7,7,7},
                        {7,7,7,7,7,7,7,7},
                        {7,7,7,7,7,7,7,7},
                        {7,7,7,7,7,7,7,7},
                        {5,5,5,5,5,5,5,5},
                        {7,7,7,7,7,7,7,7},
                        {7,7,7,7,7,7,7,7},
                        {7,7,7,7,7,7,7,7}%
                    }
                    Vec. 13
                \end{minipage}
                \begin{minipage}{0.035\textwidth}
                    \centering
                    \quad=\quad
                \end{minipage}
                \begin{minipage}{0.1\textwidth}
                    \centering
                    \IApixelimage[0.65in]{
                        {7,7,5,7,5,7,7,7},
                        {7,7,5,7,5,7,7,7},
                        {5,5,3,5,3,5,5,5},
                        {7,7,5,7,5,7,7,7},
                        {5,5,3,5,3,5,5,5},
                        {7,7,5,7,5,7,7,7},
                        {7,7,5,7,5,7,7,7},
                        {7,7,5,7,5,7,7,7}%
                    }
                    Image 3
                \end{minipage}%
            \end{figure}

            \begin{figure}[H]
                \centering
                \begin{minipage}{0.1\textwidth}
                    \centering
                    \IApixelimage[0.65in]{
                        {7,7,7,5,7,7,7,7},
                        {7,7,7,5,7,7,7,7},
                        {7,7,7,5,7,7,7,7},
                        {7,7,7,5,7,7,7,7},
                        {7,7,7,5,7,7,7,7},
                        {7,7,7,5,7,7,7,7},
                        {7,7,7,5,7,7,7,7},
                        {7,7,7,5,7,7,7,7}%
                    }
                    Vec. 4
                \end{minipage}
                \begin{minipage}{0.035\textwidth}
                    \centering
                    \quad+\quad
                \end{minipage}
                \begin{minipage}{0.1\textwidth}
                    \centering
                    \IApixelimage[0.65in]{
                        {7,7,7,7,5,7,7,7},
                        {7,7,7,7,5,7,7,7},
                        {7,7,7,7,5,7,7,7},
                        {7,7,7,7,5,7,7,7},
                        {7,7,7,7,5,7,7,7},
                        {7,7,7,7,5,7,7,7},
                        {7,7,7,7,5,7,7,7},
                        {7,7,7,7,5,7,7,7}%
                    }
                    Vec. 5
                \end{minipage}
                \begin{minipage}{0.035\textwidth}
                    \centering
                    \quad+\quad
                \end{minipage}
                \begin{minipage}{0.1\textwidth}
                    \centering
                    \IApixelimage[0.65in]{
                        {7,7,7,7,7,7,7,7},
                        {7,7,7,7,7,7,7,7},
                        {7,7,7,7,7,7,7,7},
                        {5,5,5,5,5,5,5,5},
                        {7,7,7,7,7,7,7,7},
                        {7,7,7,7,7,7,7,7},
                        {7,7,7,7,7,7,7,7},
                        {7,7,7,7,7,7,7,7}%
                    }
                    Vec. 12
                \end{minipage}
                \begin{minipage}{0.035\textwidth}
                    \centering
                    \quad+\quad
                \end{minipage}
                \begin{minipage}{0.1\textwidth}
                    \centering
                    \IApixelimage[0.65in]{
                        {7,7,7,7,7,7,7,7},
                        {7,7,7,7,7,7,7,7},
                        {7,7,7,7,7,7,7,7},
                        {7,7,7,7,7,7,7,7},
                        {7,7,7,7,7,7,7,7},
                        {5,5,5,5,5,5,5,5},
                        {7,7,7,7,7,7,7,7},
                        {7,7,7,7,7,7,7,7}%
                    }
                    Vec. 14
                \end{minipage}
                \begin{minipage}{0.035\textwidth}
                    \centering
                    \quad=\quad
                \end{minipage}
                \begin{minipage}{0.1\textwidth}
                    \centering
                    \IApixelimage[0.65in]{
                        {7,7,7,5,5,7,7,7},
                        {7,7,7,5,5,7,7,7},
                        {7,7,7,5,5,7,7,7},
                        {5,5,5,3,3,5,5,5},
                        {7,7,7,5,5,7,7,7},
                        {5,5,5,3,3,5,5,5},
                        {7,7,7,5,5,7,7,7},
                        {7,7,7,5,5,7,7,7}%
                    }
                    Image 4
                \end{minipage}%
            \end{figure}

        \section{A linear algebra analogy}
            We can draw the analogy from the abstract high-level concepts that we just looked at the change of basis procedures that we studied in class. As mentioned, above, each datapoint is a vector, and an ``expression system'' is simply a set of basis vectors that can be used to write out datapoints in an expressed form.

            By the theorem seen in class, if one has the same number of linearly independent vectors as dimensions in a space, that set of vectors forms a basis. But it we find a set of $n$ vectors, to represent datapoints in an $n$ dimensional space, we have achieved no compression, since we can use the standard basis and obtain the same storage efficiency.
            
            Ideally the dataset that we want to represent only contains vectors in a lower-dimensional subspace of the data space. That way, we can express all the data points we are interested with a smaller number of vectors.
            
            Unfortunately, real world data rarely fits a proper mathematical subspace, so this technique would not be reliable for all types of data. For example, assume all our data is on a plane within a 3-dimensional data space. With just one point that is not within the plane, we have increased the dimension of our subspace to 3, yielding not compression when expressing datapoints with new eigenvectors.
            
            Our method must take into account how ``important'' these areas of space are to decide whether it is worth reducing our compression efficiency to be able to represent only a few infrequent datapoints.
                
            % \Inote{Use our variance knowledge from example image to say that some sections of the image have less information (the constant ones) and some have more information (the ones that vary widely)}
        
        \section{Formalizing the calculation of ``high information'' eigenvectors}
            How can we use the knowledge we gained form principal component analysis, along with our linear algebra skills, to derive an algorithm for object recognition?

            The answer lies in an 1987 paper written by L. Sirbovich and M. Kirby, followed by the work of Matthew A. Turk and Alex P. Pentland, mathematicians who invented the method of ``eigenfaces'' \cite{Turk1991}. One of the earlies face detection methods, eigenfaces use rather simple math to achieve remarkable data compression that allow for mathematical descriptions of low-dimensional images.

            Let us walk through the process of creating eigenfaces and discover their utility. As we will see later, our method will generalize nicely to characterizing other detectable objects. We start with a training set of $M$ images of faces. The larger the training set, the greater the precision of the algorithm. Each image in the training set must have the same resolution. In this case we will take them to be $N \times N$ pixels ($N$ height and $N$ width) without loss of generality. Our goal is to represent each face as a vector, and we do this by placing the $N$ columns into one single column vector of $N^2$ dimensions. Each pixel in this arrangement is a variable of 256 possible color values, and our image is called an 8-bit image (8 bit images are standard, however this process is independent of our choice of $N$ \cite{Turk1991}).

            \ex{Low Resolution}{
                Consider an extremely low resolution $2 \times 2$ pixel image. We can represent this image as $2 \times 2$ matrix $A$, defined
                    \[
                        A = \IAmatrix{
                            1 & 64 \\
                            32 & 143
                        }
                    \]
            }

            Since each pixel has 256 possible values that define its color, each entry in $A$ has some value between 0 and 256. We now change $A$ into a four dimensional vector $\vec{v}$ such that
                    \[
                        \vec{v} = \IAmatrix{
                            1 \\
                            64 \\
                            32 \\
                            143
                        }
                    \]

            Notice that this operation does not change the information contained in each image, it simply gives us an object that is easier to work with. Doing this operation on each of our $M$ images, we can now call refer to the $a^{th}$ face vector as $\Gamma_a$, where $1 \leq a \leq M$. Having defined our vectors, let us turn to principal component analysis.

            The motivation for using principal component analysis is to find what mathematicians Turk and Pentland referred to as ``face space.'' The face vectors live in large dimensional spaces ($65,536$-dimensional space for a $256$ by $256$ image). The goal at this stage is to reduce our large input vector space to a lower dimensional subspace. We will do this by taking the eigenvectors of a covariance matrix. From there, we will be able to write any face as a linear combination of these eigenvectors.

            In defining this matrix, let us revisit our definition of covariance and adapt it from a mathematically defined random variable to one comprised of samples.

            \dfn{Sample Covariance}{
                In order to construct the covariance matrix, we need to define \textbf{covariance} for samples of random variable that holds some true distribution that we lack knowledge of.
                
                \smallskip Let $\sigma(X, Y)$ be the covaraince between r.v.'s $X$ and $Y$, and let $X_a$ be the $a^{th}$ sample of the $X$ random variable, and $\IAmean{X}$ the sample mean (weighted average value of r.v. based on number of appearances in dataset).
                
                \smallskip Mathematically, sample covaraince is then defined as
                    \[
                        \sigma(X, Y) = \frac{1}{M} \sum_{a=1}^M (X_a - \IAmean{X})(Y_a - \IAmean{Y})
                    \]
            }

            \dfn{Covariance Matrix}{
                The covariance matrix, then, is a matrix that measures the covariance matrix between each pair of variables. We denote the variables as $x_1,\dots,x_p$. If we have $N^2$ variables, our covariance matrix is a $N^2 \times N^2$ matrix
                    \[
                        C = \IAmatrix{
                            \sigma_{11} & \sigma_{12} & ...    & \sigma_{1p} \\
                            \sigma_{21} & \sigma_{22} &        &             \\
                            \vdots      &             & \ddots &             \\
                            \sigma_{p1} &             &        & \sigma _{pp}
                        }
                    \]
                where the entry $\sigma_{ij}$ represents the covariance between the $i^{th}$ and $j^{th}$ variables.
            }

            In the case of eigenfaces, the $p$ variables correspond to each of the $N^2$ pixels in an image. Thus, $p = N^2$ for our application, where $N$ is the width and height of an image \cite{Janakiev2018}.

            Having defined our covariance matrix, we must now discuss why we are taking the eigenvectors of this particular matrix to be our eigenfaces. The answer lies in the fact that the eigenvectors of the covariance matrix are the directions of maximum variance in the data. In other words, the eigenvectors of the covariance matrix are the directions of maximum information. % \Inote{In the future add reference to information theory here and before explain how more variance would naturally lead to more information entropy.}

            We calculate the eigenvectors so that we simplify a relation based on matrix multiplication, to a relation defined by scalar multiplication, so that we may write any face as a linear combination of eigenvectors.

            Taking an eigenvector $u$, $Cu=\lambda_u u$. Notice that if $\lambda_u$ is large, then the amount of information contained in the direction of $u$ is large. Thus, we can use the eigenvectors of the covariance matrix to represent our faces in a lower dimensional space by discarding the low eigenvalue ones and projecting all images to the subspace spanned by the remaining eigenvectors.

            Eigenfaces are sometimes referred to as ``ghost-like'' images \cite{Dusenberry2015}, given their slightly vague features. Intuitively, we can understand each eigenface as a sort of general face, where a greater eigenvalue implies a more general face. By combining general faces with more specific faces, we can obtain an approximation of any other face (that is reasonably similar to our training set).

            \begin{figure}[H]
                \centering
                \includegraphics[width=0.5\textwidth]{eigenfaces.jpg}
                \caption{"Ghost-like" eigenfaces \cite{Dusenberry2015}}
            \end{figure}

            Before we discuss the eigenvalues, however, we must derive some more properties of $C$ that will ultimately help us condense our data.

            Firstly, given the large size of $N^2$, it is conventient for us to find a more condensed description of the covariance matrix.

            \thm{Simplified Covariance Matrix}{
                Let $C$ be a $p \times p$ covariance matrix where each matrix entry $c_{ij} = \sigma_{ij}$. Define the variance
                    \[\Phi_a = P_a - \IAmean{P},\]
                where
                    \[
                        P_a = \IAmatrix{
                            x_{1a} \\
                            x_{2a} \\
                            \vdots \\
                            x_{pa}
                        }, \quad 1 \leq a \leq M
                    \]
                
                \smallskip Then, the covariance matrix can be written as
                    \[
                        C = \frac{1}{M} \sum_{a=1}^M \Phi_a \Phi_a^T
                    \]
            }

            Recall that each vector has $N^2$ dimensions, corresponding to pixels, and that our training set has $M$ vectors. The first index $p$ marks the $p^{th}$ pixel, and the second index $a$ marks the $a^{th}$ image in the training set. 

            \begin{myproof}
                We can start the proof of this thoerem with our matrix
                \[
                    C = \IAmatrix{
                        \sigma_{11} & \sigma_{12} & ...    & \sigma_{1p} \\
                        \sigma_{21} & \sigma_{22} &        &             \\
                        \vdots      &             & \ddots &             \\
                        \sigma_{p1} &             &        & \sigma _{pp}
                    }
                \]

                For any entry, $c_{ij}=\sigma_{ij}$. From the definition of covariances, each entry 
                    \[
                        c_{ij} = \frac{1}{M} \sum_{a=1}^M \left(x_{ia} - \IAmean{x}_i\right) \left(x_{ja} - \IAmean{x}_j\right)
                    \]
                
                Since matrices add linearly, we can factor out the summation $\frac{1}{M} \sum_{a=1}^M$. Now we can write the whole matrix,
                    \[
                        C = \frac{1}{M} \sum_{a=1}^M \IAmatrix{
                            \left(x_{1a} - \IAmean{x}_1\right) \left(x_{1a} - \IAmean{x}_1\right) & \dotsc  & \left(x_{1a} - \IAmean{x}_1\right) \left(x_{pa} - \IAmean{x}_p\right) \\
                            \vdots & \ddots & \\
                            \left(x_{pa} - \IAmean{x}_p\right) \left(x_{1a} - \IAmean{x}_1\right) &  & \left(x_{pa} - \IAmean{x}_p\right) \left(x_{pa} - \IAmean{x}_p\right)
                        }
                    \] 
                
                Now consider the term $\Phi_a \Phi_a^T = (P_a - \IAmean{P})(P_a - \IAmean{P})^T$, as defined in the theorem. Since the transpose operator is distributive,
                \begin{gather*}
                    (P_a - \IAmean{P})(P_a - \IAmean{P})^{T} =
                        \left(
                            \IAmatrix{
                                x_{1a} \\
                                x_{2a} \\
                                \vdots \\
                                x_{pa}
                            } - \IAmean{P}
                        \right)
                        \left(
                            \IAmatrix{
                                x_{1a} & x_{2a} & \dotsc & x_{pa}
                            } - \IAmean{P}^T
                        \right)
                    \\
                    = \left(
                        \IAmatrix{
                            x_{1a} \\
                            x_{2a} \\
                            \vdots \\
                            x_{pa}
                        } - \IAmatrix{
                            \IAmean{x}_1 \\
                            \IAmean{x}_2 \\
                            \vdots \\
                            \IAmean{x}_p
                        }
                    \right)
                    \left(
                        \IAmatrix{
                            x_{1a} & x_{2a} & \dotsc & x_{pa}
                        } - \IAmatrix{
                            \IAmean{x}_1 & \IAmean{x}_2 & \dotsc & \IAmean{x}_p
                        }
                    \right)
                    \\
                    = \IAmatrix{
                        x_{1a} - \IAmean{x}_1 \\
                        x_{2a} - \IAmean{x}_2 \\
                        \vdots \\
                        x_{pa} - \IAmean{x}_p
                    } \IAmatrix{
                        x_{1a} - \IAmean{x}_1 & x_{2a} - \IAmean{x}_2 & \dotsc & x_{pa} - \IAmean{x}_p
                    }
                \end{gather*}
            

                Multiplying these two matrices, we get that
                    \[
                        \Phi_a {\Phi_a}^T = \IAmatrix{
                            \left(x_{1a} - \IAmean{x}_1\right)\left(x_{1a} - \IAmean{x}_1\right) & \dotsc & \left(x_{1a} - \IAmean{x}_1\right) \left(x_{pa} - \IAmean{x}_p\right) \\
                            \vdots & \ddots & \\
                            \left(x_{pa} - \IAmean{x}_p\right) \left(x_{1a} - \IAmean{x}_1\right) & & \left(x_{pa} - \IAmean{x}_p\right) \left(x_{pa} - \IAmean{x}_p\right)
                        }
                    \]
               
                    Notice that any entry $A_{ij}$ in this final matrix resembles the formula for the covariance between the $i^{th}$ and $j^{th}$ variable. The only component missing is the summation on all values of $a$. Thus we can write,
                    \[
                        C = \frac{1}{M} \sum_{a=1}^M \Phi_a {\Phi_a}^T
                    \]
            \end{myproof}

            %\href{https://builtin.com/data-science/step-step-explanation-principal-component-analysis}{REFERENCE PRINCIPAL COMPONENT ANALYSIS} \Inote{Cite}

            In the eigenface case, we wish to measure the difference between the face vectors $\Gamma_a$ and the average face. We define the average face $\IAmean{\Gamma}$ as follows,
                \[
                    \IAmean{\Gamma} = \frac{1}{M} \sum_{a=1}^M \Gamma_a
                \]

            We can write the difference $\Phi_a$ between the average face and the $a^{th}$ face,
                \[
                    \Phi_a = \Gamma_a - \IAmean{\Gamma}
                \]

            
            %\href{https://nzmaths.co.nz/category/glossary/variance-discrete-random-variable\#:\textasciitilde:text=A\%20measure\%20of\%20spread\%20for,2\%20or\%20\%CF\%832x.}{Hello}
            Following the rules of matrix multiplication, we can also rewrite our definition
                \[
                    C = \frac{1}{M} \sum_{a=1}^M \Phi_a {\Phi_a}^T
                \]
                \[
                    C = AA^T
                \]
            Where $A$ is a matrix with columns $\IAmatrix{\Phi_1 & \Phi_2 & \cdots & \Phi_M}$.
           
            Now we express the eigenvectors $u_i$ and eigenvalues $\lambda_i$ of our covariance matrix as
                \[
                    \lambda _{i} u_{i} =AA^{T} u_{i}
                \]

            However, $AA^T$ is an $N^2 \times N^2$ matrix, which for images of any reasonable size is going to be very large.

            Consider instead the eigenvalues of $A^T A$, 
                \[
                    A^TAv_{i} = \lambda_i v_i
                \]
            
            Multiplying both sides of our equation by $A$ we get
                \[
                    AA^TAv_{i} = \lambda_i Av_{i}
                \]

            Therefore, for any eigenvector $v_i$ of $A^TA$, $u_{i}=Av_{i}$ is an eigenvector of $AA^T$\cite{wiki:Eigenface}.
             
            We can now focus on the eigenvectors of $A^TA$ instead. The advantage of effectively reordering the $A$ and $A^T$ is that out new matrix is an $M \times M$ matrix. $M$ corresponds to the number of images in the training set, which in most cases is much smaller than the dimension of the resolution of the image. Therefore we have overcome the main obstacle in our path, and from this point on it will be much easier to compute the eigenfaces.
            
            We can write each face in the training set as a linear combination of our $M$ eigenfaces
                \[
                    \Gamma_a = c_{a1} u_1 + c_{a2} u_2 + \dots + c_{an} u_n \\
                     = \sum_{i=1}^M c_{in} u_n
                \] 

            Similarly, we can describe a face image not in the training set as a combination of weights and eigenvectors, by solving the above equation for the weights $c_{a1},\dots,c_{an}$. With this last step we have a complete picture of the facial recognition process.

            We have walked through this process looking for facial recognition capabilities, as that was the historically taken route, but this use of eigenvectors is actually much more general. We can apply this method to a variety of data recognition processes, including the others mentioned in this report.

            Fingerprints, for example, are a very common form of biometric identification. The process of identifying a fingerprint is very similar to the facial recognition process.
            
            With fingerprints, images are usually black or white (no gray), so we go from an 8-bit image to a binary 1-bit image. From there, we follow the same process as we did with facial recognition. We can use eigenvectors to represent ``general fingerprints'', and we can identify new fingerprints by the weights it takes to represent them as a linear combination of eigenvectors. These ``eigen-fingerprints'' can be used in the same way as the eigenfaces to recognize fingerprints.

            \begin{figure}[H]
                \centering
                \includegraphics[width=0.5\textwidth]{eigenfingerprints.png}
                \caption{Example eigen-fingerprints and eigen-digitprints; digitprints are simply another form of fingerprint image \cite{Pavesic2022}}
            \end{figure}

            Notice how these eigen-fingerprints get more detailed and granular as we increase the eigenvalue (in the image higher eigenvalues correspond with lower values of $i$). By combining these eigenvectors, we can recreate any reasonably similar fingerprint. This technology has direct applications in security, where a scanned fingerprint can be projected on to the subspace spanned by the chosen eigenvectors (typically less than the dimension of the original fingerprint image) and compared to a list of weights in the database of approved fingerprints. If the weights are very similar (less than a certain threshold of difference) with a fingerprint in the database, then the access is granted.

            % Example calculation of covariance matrix
            % \ex{Covariance Matrix}{
            %     Let us consider a simple example of a covariance matrix. Suppose we have two variables, $x$ and $y$, and we have two samples of each variable. We can represent these variables as
            %         \[
            %             \vec{x} = \IAmatrix{
            %                 1 \\
            %                 2
            %             }
            %         \]
            %         \[
            %             \vec{y} = \IAmatrix{
            %                 3 \\
            %                 4
            %             }
            %         \]
            %     The sample mean of $x$ is
            %         \[
            %             \IAmean{x} = \frac{1}{2} \sum_{i=1}^2 x_i = \frac{1}{2} (1 + 2) = \frac{3}{2}
            %         \]
            %     and the sample mean of $y$ is
            %         \[
            %             \IAmean{y} = \frac{1}{2} \sum_{i=1}^2 y_i = \frac{1}{2} (3 + 4) = \frac{7}{2}
            %         \]
            %     The covariance matrix is then
            %         \[
            %             C = \IAmatrix{
            %                 \sigma_{11} & \sigma_{12} \\
            %                 \sigma_{21} & \sigma_{22}
            %             }
            %         \]
            %     where $\sigma_{11}$ is the covariance between $x$ and $x$, $\sigma_{12}$ is the covariance between $x$ and $y$, $\sigma_{21}$ is the covariance between $y$ and $x$, and $\sigma_{22}$ is the covariance between $y$ and $y$. We can calculate these values as
            %         \[
            %             \sigma_{11} = \frac{1}{2} \sum_{i=1}^2 (x_i - \IAmean{x})(x_i - \IAmean{x}) = \frac{1}{2} \sum_{i=1}^2 (x_i - \frac{3}{2})(x_i - \frac{3}{2}) = \frac{1}{2} \sum_{i=1}^2 (x_i^2 - 3x_i + \frac{9}{4}) = \frac{1}{2} \sum_{i=1}^2 (x_i^2 - 3x_i + \frac{9}{4}) = \frac{1}{2} (1^2 - 3 \cdot 1 + \frac{9}{4} + 2^2 - 3 \cdot 2 + \frac{9}{4}) = \frac{1}{2} (1 - 3 + \frac{9}{4} + 4 - 6 + \frac{9}{4}) = \frac{1}{2} (\frac{9}{4} + \frac{9}{4}) = \frac{9}{2}
            %         \]
            %         \[
            %             \sigma_{12} = \frac{1}{2} \sum_{i=1}^2 (x_i - \IAmean{x})(y_i - \IAmean{y}) = \frac{1}{2} \sum_{i=1}^2 (x_i - \frac{3}{2})(y_i - \frac{7}{2}) = \frac{1}{2} \sum_{i=1}^2 (x_i - \frac{3}{2})(y_i - \frac{7}{2}) = \frac{1}{2} \sum_{i=1}^2 (x_i - \frac{3}{2})(y_i - \frac{7}{2}) =
            %         \]
            % }
    
    \chapter{Uses Beyond Compression}
    We mainly discussed how PCA allows us to reduce the dimensions of our data space and store data more efficiently, but there are more uses for this beyond compression. Having a simpler representation for our data allows us to perform more complex operations on it.

    \section{Classification}
    We can use PCA to perform classification on our data. After we have chosen our eigenvectors based on our initial dataset, and have manually classified the original ``training'' data into categories, we can use Linear Classification \cite{Milon2019} to classify new data into the preexisting categories.

    This process consists of projecting the new datapoint to our lower-dimension subspace and then making a category decision based on how much weight each basis vector has in the new datapoint.

    \section{Recognition}
    In cases were there are no discrete categories defined during training time, we can use dimensionality reduction to more easily (and faster) compare a new datapoint to a list of known datapoints that continually gets updates.

    In the case of facial recognition, we can use PCA to reduce the dimensionality of our data and then use a distance metric to compare the new datapoint to the known datapoints. The datapoint with the smallest distance to the new datapoint is the most similar and is therefore the most likely to be the same person.

    % \chapter{Discussion}
    % \section{Is this software Anglo-centric?}
    % \blindtext[1]
    
    \pagebreak
    
    % ================ %
    % == References == %
    % ================ %
    \bibliographystyle{alpha}
    \bibliography{sample}
    
\end{document}